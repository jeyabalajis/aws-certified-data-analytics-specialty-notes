{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>AWS Certified Data Analytics Specialty - Concept Notes</p> <p>Welcome to this portal! You can find useful information for successful preparation of AWS Certified Data Analytics Specialty examination.</p> <p>The content is organized by various categories, which makes it easy for you to focus on specific categories that you need.</p> <p>Happy learning!</p>"},{"location":"collection/collection.html","title":"Collection","text":""},{"location":"collection/collection.html#ongoing-collection","title":"Ongoing collection","text":"<ul> <li>If the data streams must be stored for at least 3 days, you must use Kinesis Data Streams which has a configurable data retention of between 1 and 365 days.</li> <li>The KPL has the mechanisms in place for retry and batching, as well as asynchronous mode. </li> <li>The Kinesis agent is meant to retrieve server logs with just configuration files.</li> <li>Lambda can be a consumer for Kinesis Data Streams (KDS) but not for Kinesis Data Firehose (KDF). However, for transformation needs, Lambda can be used in conjunction with KDF.</li> </ul>"},{"location":"collection/collection.html#kinesis-data-streams","title":"Kinesis Data Streams","text":"<ul> <li>KDS with provisioned capacity does not scale as the load increases. </li> <li>With on-demand scaling, KDS can be used for scaling with increasing data, but if the processing times are in minutes (not seconds), a different approach (such as SQS) could be required.</li> <li>For ongoing data collection and when configurable data retention is required, choose Kinesis Data Streams over Kinesis Data Firehose.</li> <li>KDS can replay data to handle downstream failures.</li> </ul> <p>Kinesis Data Stream allows Data replays (in the same order). SQS does not allow data replays, and DynamoDB would allow to replay some data, but it\u2019d be difficult to get some ordering constraints working as well as well as enable real time use cases.</p> <p></p> <p></p> <p></p>"},{"location":"collection/collection.html#kinesis-data-firehose","title":"Kinesis Data Firehose","text":"<ul> <li>Using KDF, data can be sinked into S3, Redshift, ElasticSearch or Splunk.</li> </ul>"},{"location":"collection/collection.html#data-migration-service-dms","title":"Data Migration Service (DMS)","text":"<ul> <li>DMS support Apache Parquet format when migrating data to S3.</li> </ul>"},{"location":"collection/collection.html#cost","title":"Cost","text":"<ul> <li>KDS: You pay per shard hour and per PUT payload unit. Optionally, there are fees associated with extended data retention and enhanced fan-out, if you choose to use those features. </li> <li>KDF: You pay for the volume of data you ingest using the service and for any data format conversions.</li> <li>DMS: You pay for compute resources (depending on instance type) used during the migration process and any additional log storage. There are also potential data transfer fees.</li> <li>Glue: With AWS Glue, you pay an hourly rate, billed by the second, for crawlers (discovering data) and ETL jobs (processing and loading data).</li> </ul>"},{"location":"collection/collection.html#delivery-guarantees","title":"Delivery Guarantees","text":"<ul> <li>Amazon SQS (FIFO) and Amazon DynamoDB streams provide exactly-once delivery. All other AWS services provide at-least-once.</li> <li>All services listed (i.e. KDS, DynamoDB Streams, Amazon MSK and SQS FIFO) support exactly once delivery, except for Kinesis Data Firehose (KDF) and Amazon SQS (Standard).</li> </ul>"},{"location":"collection/collection.html#transform-and-filter-data-during-collection-process","title":"Transform and filter data during collection process","text":"<ul> <li>Use Lambda to transform data that is in KPL or GZIP into JSON or CSV for Kinesis Data Analytics.<ul> <li>Data Enrichment</li> <li>String Transformation</li> <li>Data Filtering </li> </ul> </li> <li>Use KDF Transformation to batch, compress, and encrypt data before loading it.</li> <li>DMS can be used for schema conversions. <ul> <li>It also supports Apache Parquet format when migrating data to Amazon S3.</li> </ul> </li> </ul>"},{"location":"collection/database_migration_service.html","title":"Database Migration Service","text":"<p>AWS DMS provides one-time migration and continuous replication of your database records and data structures to AWS.</p> <p>AWS Database Migration Service (AWS DMS) can be used to migrate data  - from your database that is      - on-premises,      - on an Amazon Relational Database Service (Amazon RDS) DB instance,      - or in a database on an Amazon Elastic Compute Cloud (Amazon EC2) instance  - to a database on an AWS service.</p> <p>With AWS DMS, you can create a task that captures ongoing changes after you complete your initial migration to a supported target data store. This process is called ongoing replication or change data capture (CDC). </p>"},{"location":"collection/database_migration_service.html#scalability","title":"Scalability","text":"<ul> <li>AWS DMS uses Amazon EC2 instances as the replication instance. </li> <li>You can scale up or down your replication instance, depending on utilization.</li> <li>You have the option of enabling Multi-AZ which provides a replication stream that is fault-tolerant through redundant replication servers.</li> </ul>"},{"location":"collection/database_migration_service.html#sources-and-targets","title":"Sources and Targets","text":""},{"location":"collection/database_migration_service.html#sources","title":"Sources","text":"<ul> <li>Oracle</li> <li>Microsoft</li> <li>MySQL</li> <li>PostgreSQL</li> <li>IBM DB2</li> <li>MongoDB</li> <li>Document DB</li> <li>SAP ASE</li> </ul>"},{"location":"collection/database_migration_service.html#targets","title":"Targets","text":"<ul> <li>DynamoDB</li> <li>S3</li> <li>Kinesis Data Streams</li> <li>Apache Kafka</li> <li>Document DB</li> <li>Neptune</li> <li>Redis</li> <li>Babelfish</li> </ul>"},{"location":"collection/database_migration_service.html#from-rdbms-to-s3","title":"From RDBMS to S3","text":"<ul> <li>The solution streams new and changed data into Amazon S3 using AWS Database Migration Service (AWS DMS). </li> <li>It reads historical data from source systems, such as relational database management systems, data warehouses, and NoSQL databases, at any desired interval. </li> <li>The solution streams new and changed data into Amazon S3.</li> </ul>"},{"location":"collection/dynamodb_streams.html","title":"DynamoDB Streams","text":""},{"location":"collection/dynamodb_streams.html#salient-features","title":"Salient Features","text":"<ul> <li>Each stream record appears exactly once in the stream (Exactly-Once-Guarantee).</li> <li>For each item that is modified in a DynamoDB table, the stream records appear in the same sequence as the actual modifications to the item (Ordered Sequence).</li> <li>To work with database tables and indexes, your application must access a DynamoDB endpoint. To read and process DynamoDB Streams records, your application must access a DynamoDB Streams endpoint in the same Region.</li> </ul>"},{"location":"collection/dynamodb_streams.html#reading-and-processing-a-stream","title":"Reading and Processing a Stream","text":"<ul> <li>To read and process a stream, your application must connect to a DynamoDB Streams endpoint and issue API requests.</li> <li>A stream consists of stream records. </li> <li>Each stream record represents a single data modification in the DynamoDB table to which the stream belongs. </li> <li>Each stream record is assigned a sequence number, reflecting the order in which the record was published to the stream.</li> <li>Stream records are organized into groups, or shards. Each shard acts as a container for multiple stream records, and contains information required for accessing and iterating through these records. </li> <li>The stream records within a shard are removed automatically after 24 hours.</li> <li>Shards are ephemeral: They are created and deleted automatically, as needed. </li> <li>Any shard can also split into multiple new shards; this also occurs automatically. (It's also possible for a parent shard to have just one child shard.) </li> <li>A shard might split in response to high levels of write activity on its parent table, so that applications can process records from multiple shards in parallel.</li> <li>If you disable a stream, any shards that are open will be closed. The data in the stream will continue to be readable for 24 hours.</li> </ul>"},{"location":"collection/dynamodb_streams.html#kinesis-adapter","title":"Kinesis Adapter","text":"<ul> <li>Because shards have a lineage (parent and children), an application must always process a parent shard before it processes a child shard. </li> <li>This helps ensure that the stream records are also processed in the correct order.</li> <li>If you use the DynamoDB Streams Kinesis Adapter, this is handled for you.</li> <li>The DynamoDB Streams Kinesis Adapter acts as a transparent layer between the KCL and the DynamoDB Streams endpoint, so that the code can fully use KCL rather than having to make low-level DynamoDB Streams calls.</li> </ul>"},{"location":"collection/dynamodb_streams.html#data-retention-limit-for-dynamodb-streams","title":"Data retention limit for DynamoDB Streams","text":"<ul> <li>All data in DynamoDB Streams is subject to a 24-hour lifetime. </li> <li>You can retrieve and analyze the last 24 hours of activity for any given table. </li> <li>However, data that is older than 24 hours is susceptible to trimming (removal) at any moment.</li> </ul>"},{"location":"collection/dynamodb_streams.html#kinesis-data-streams-kds-for-dynamodb","title":"Kinesis Data Streams (KDS) for DynamoDB","text":"<ul> <li>You can use Amazon Kinesis Data Streams to capture changes to Amazon DynamoDB.</li> <li>Kinesis Data Streams captures item-level modifications in any DynamoDB table and replicates them to a Kinesis data stream. </li> <li>Your applications can access this stream and view item-level changes in near-real time. </li> <li>You can continuously capture and store terabytes of data per hour. </li> <li>You can take advantage of longer data retention time\u2014and with enhanced fan-out capability, you can simultaneously reach two or more downstream applications. </li> <li>Other benefits include additional audit and security transparency.</li> <li>Kinesis Data Streams also gives you access to Amazon Kinesis Data Firehose and Amazon Kinesis Data Analytics. </li> <li>These services can help you build applications that power real-time dashboards, generate alerts, implement dynamic pricing and advertising, and implement sophisticated data analytics and machine learning algorithms.</li> </ul> <p>You can only stream data from DynamoDB to Kinesis Data Streams in the same AWS account and AWS Region as your table. You can only stream data from a DynamoDB table to one Kinesis data stream.</p>"},{"location":"collection/glue_etl.html","title":"Glue ETL","text":""},{"location":"collection/glue_etl.html#scheduling","title":"Scheduling","text":"<ul> <li>Minimum Precision for Time Based Scheduling is 5 minutes.</li> </ul>"},{"location":"collection/glue_etl.html#glue-for-streaming-data","title":"Glue for Streaming Data","text":"<ul> <li>While Glue can process micro-batches, it does not handle streaming data. </li> <li>If your use case requires you to ETL data while you stream it in, <ul> <li>you can perform the first leg of your ETL using Amazon Kinesis, Amazon Kinesis Data Firehose, or Amazon Kinesis Data Analytics. </li> <li>Then store the data in either Amazon S3 or Amazon Redshift and </li> <li>trigger an AWS Glue ETL job to pick up that dataset and continue applying additional transformations to that data.</li> </ul> </li> </ul>"},{"location":"collection/glue_etl.html#scalability","title":"Scalability","text":"<ul> <li>AWS Glue uses a scale-out Apache Spark environment to load your data into its destination. </li> <li>To scale out, you specify the number of DPUs (data processing units) that you want to allocate to your ETL jobs.</li> <li>A data processing unit (DPU) is a relative measure of processing power that consists of vCPUs and memory. </li> <li>To improve the job execution time, you can enable job metrics in AWS Glue to estimate the number of data processing units (DPUs) that can be used to scale out an AWS Glue job.</li> </ul>"},{"location":"collection/glue_etl.html#glue-catalog","title":"Glue Catalog","text":"<ul> <li>If AWS Glue doesn\u2019t find a custom classifier that fits the input data format with 100 percent certainty, then AWS Glue invokes the built-in classifiers. </li> <li>The built-in classifier returns either certainty=1.0 if the format matches, or certainty=0.0 if the format doesn't match. </li> <li>If no classifier returns certainty=1.0, then AWS Glue uses the output of the classifier that has the highest certainty.</li> <li>Updating the Glue Catalog takes some time. If up-to-date results are required immediately, Apache Presto can be used to query all the datasets in place.</li> </ul> <p>If no classifier returns a certainty of higher than 0.0, then AWS Glue returns the default classification string of UNKNOWN. </p> <p>Grok is a tool that is used to parse textual data given a matching pattern. A grok pattern is a named set of regular expressions (regex) that are used to match data one line at a time. AWS Glue uses grok patterns to infer the schema of your data. When a grok pattern matches your data, AWS Glue uses the pattern to determine the structure of your data and map it into fields.</p>"},{"location":"collection/glue_etl.html#handling-new-partitions-in-s3-bucket-data-lake","title":"Handling New Partitions in S3 Bucket (Data Lake)","text":"<ul> <li>When the job finishes, view the new partitions on the console right away, without having to rerun the crawler. </li> <li>You can enable this feature by adding a few lines of code to your ETL script. </li> <li>The code uses the <code>enableUpdateCatalog</code> argument to indicate that the Data Catalog is to be updated during the job run as the new partitions are created.</li> </ul>"},{"location":"collection/glue_etl.html#sample-code-python","title":"Sample Code (Python):","text":"<pre><code>additionalOptions = {\"enableUpdateCatalog\": True}\nadditionalOptions[\"partitionKeys\"] = [\"region\", \"year\", \"month\", \"day\"]\n</code></pre>"},{"location":"collection/kinesis_data_analysis.html","title":"Kinesis Data Analysis","text":""},{"location":"collection/kinesis_data_analysis.html#integration-with-kinesis-firehose","title":"Integration with Kinesis Firehose","text":"<ul> <li>Amazon Kinesis Data Analytics can query data in a Kinesis Data Firehose delivery stream in near-real time using SQL.</li> <li>A sliding window analysis is appropriate for determining trends in the stream.</li> </ul>"},{"location":"collection/kinesis_data_analysis.html#kinesis-data-analytics-application","title":"Kinesis Data Analytics Application","text":"<ul> <li>You cannot configure either Amazon DynamoDB or AWS Glue Data Catalog as a reference data source for a Kinesis data analytics application.</li> <li>An Amazon Kinesis Data Analytics application can receive input from a single streaming source and, optionally, use one reference data source to enrich the data coming in from streaming sources. You must store reference data as an object in your Amazon S3 bucket.</li> <li>Suppose that you want to refresh the data after Kinesis Data Analytics creates the in-application reference table, you can explicitly call the UpdateApplication API.</li> </ul>"},{"location":"collection/kinesis_data_firehose.html","title":"Kinesis Data Firehose","text":"<p>Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Amazon OpenSearch Serverless, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic.</p> <ul> <li>Fully managed, send to S3, Splunk, Redshift, ElasticSearch</li> <li>Serverless data transformations with Lambda</li> <li>Near real time (lowest buffer time is 1 minute)</li> <li>Automated Scaling</li> <li>No data storage</li> </ul> <p>KDF is more managed than KDS, but only near-real-time. For real-time, use KDS (but need to write custom code)</p>"},{"location":"collection/kinesis_data_firehose.html#buffering-logic","title":"Buffering Logic","text":"<ul> <li>Firehose accumulates records in a buffer</li> <li>The buffer is flushed based on time and size rules (lowest buffer time is 1 minute)</li> <li>Buffer Size (ex: 32MB): if that buffer size is reached, it\u2019s flushed</li> <li>Buffer Time (ex: 2 minutes): if that time is reached, it\u2019s flushed</li> <li>Firehose can automatically increase the buffer size to increase throughput<ul> <li>High throughput =&gt; Buffer Size will be hit</li> <li>Low throughput =&gt; Buffer Time will be hit</li> </ul> </li> </ul>"},{"location":"collection/kinesis_data_firehose.html#resilience","title":"Resilience","text":"<ul> <li>Highly resilient &amp; Serverless.</li> <li>Kinesis Data Firehose runs in a serverless mode, and takes care of host degradations, Availability Zone availability, and other infrastructure related issues by performing automatic migration.</li> </ul>"},{"location":"collection/kinesis_data_firehose.html#record-format-conversion","title":"Record Format Conversion","text":"<ul> <li>A deserializer to read the JSON of your input data. You can choose one of two types of deserializers: Apache Hive JSON SerDe or OpenX JSON SerDe.</li> <li>A schema to determine how to interpret that data \u2013 Use AWS Glue to create a schema in the AWS Glue Data Catalog. - Kinesis Data Firehose then references that schema and uses it to interpret your input data.</li> <li>A serializer to convert the data to the target columnar storage format (Parquet or ORC) \u2013 You can choose one of two types of serializers: ORC SerDe or Parquet SerDe.</li> </ul>"},{"location":"collection/kinesis_data_firehose.html#streaming-etl","title":"Streaming ETL","text":"<p>Kinesis Data Firehose and Kinesis Data Analytics are integrated with AWS Lambda to perform streaming ETL.</p> <p>For sub-second analytics, KDF --&gt; S3 --&gt; Athena is useful. But for near-real time analytics, use KDF --&gt; Amazon OpenSearch service (this provides near-real time analytics capability to consumers)</p>"},{"location":"collection/kinesis_data_firehose.html#kinesis-agent","title":"Kinesis Agent","text":"<ul> <li>You can install the Kinesis Agent on Linux-based server environments such as web servers, log servers, and database servers. - After installing the agent, configure it by specifying the files to monitor and the delivery stream for the data. </li> <li>After the agent is configured, it durably collects data from the files and reliably sends it to the delivery stream.</li> </ul>"},{"location":"collection/kinesis_data_firehose.html#use-cases","title":"Use Cases","text":"<ul> <li>Cloud Watch Logs --&gt; Subscription Filter --&gt; Kinesis Data Firehose (optional Lambda Transform) --&gt; Amazon ES/S3/Splunk</li> </ul> <p>For real time, use Lambda instead of Kinesis Data Firehose.</p> <p>For real time analytics, use Kinesis data Streams in conjunction with Kinesis Data Analytics.</p>"},{"location":"collection/kinesis_data_firehose.html#kinesis-data-firehose-to-s3","title":"Kinesis Data Firehose to S3","text":"<ul> <li>Kinesis Data Firehose can be configured with custom prefixes and dynamic partitioning. </li> <li>Using these features, you can configure the Amazon S3 keys and set up partitioning schemes that better support your use case.</li> <li>You can also use partition projection with these partitioning schemes and configure them accordingly.</li> </ul> <p>For example, you could use the custom prefix feature to get Amazon S3 keys that have ISO formatted dates instead of the default yyyy/MM/dd/HH scheme. You can also combine custom prefixes with dynamic partitioning to extract a property like customer_id from Kinesis Data Firehose messages.</p>"},{"location":"collection/kinesis_data_streams.html","title":"Kinesis Data Streams","text":""},{"location":"collection/kinesis_data_streams.html#ingress-limitations","title":"Ingress Limitations","text":"<ul> <li>Each shard can support writes up to 1,000 records per second, up to a maximum data write total of 1 MB per second. </li> <li>Each PutRecords request can support up to 500 records. </li> <li>Each record in the request can be as large as 1 MB, up to a limit of 5 MB for the entire request, including partition keys.<ul> <li>The maximum size of a data blob (the data payload before Base64-encoding) is 1 megabyte (MB).</li> </ul> </li> </ul>"},{"location":"collection/kinesis_data_streams.html#kinesis-producer-library","title":"Kinesis Producer Library","text":"<ul> <li>The Amazon Kinesis Producer Library (KPL) performs many tasks common to creating efficient and reliable producers for Amazon Kinesis. By using the KPL, customers do not need to develop the same logic every time they create a new application for data ingestion.</li> <li>RecordMaxBufferedTime: Maximum amount of time a record is buffered. Large value provides better performance, but causes delay.</li> <li>On the same host, KPL achieves around 800 times greater throughput. </li> <li>Batching, retries, and monitoring - implementing these by every producer is cumbersome, that is why KPL was created.</li> </ul>"},{"location":"collection/kinesis_data_streams.html#what-kpl-handles","title":"What KPL handles:","text":"<ul> <li>Multi-threading</li> <li>Batching records, instead of sending records one by one (You can save on HTTP Headers space; instead of headers being sent in every request, batching saves on that bandwidth)</li> <li>With Batching (i.e. use of PutRecords, some records will partially fail and must be retried.)</li> <li>Retry of failed response from Amazon Kinesis API. This also requires exponential backoff handling.</li> <li>if you retried failed records without any backoff, then you might end up spamming a shard even though it\u2019s already having problems.</li> </ul> <p>In a cost-optimal architecture, we all want to have as few shards as possible, but still be able to ingest all data promptly at peak traffic.</p> <p>To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. It also must deal with the transient failures inherent to any network application and perform retries as needed. And in a large-scale application, monitoring is needed to allow operators to diagnose and troubleshoot any issues that arise.</p>"},{"location":"collection/kinesis_data_streams.html#kpl-features","title":"KPL Features","text":"<ul> <li>Batching of puts using PutRecords</li> <li>Tracking of record age and enforcement of maximum buffering times</li> <li>Per-shard record aggregation (the Aggregator)</li> <li>Retries in case of errors, with ability to distinguish between retryable and non-retryable errors</li> <li>Per-shard rate limiting to prevent excessive and pointless spamming</li> <li>Useful metrics and a highly efficient CloudWatch client</li> </ul>"},{"location":"collection/kinesis_data_streams.html#aggregation","title":"Aggregation","text":"<ul> <li>Record aggregation allows customers to combine multiple records into a single Kinesis Data Streams record. This allows customers to improve their per shard throughput.</li> <li>You want a binary format that\u2019s unambiguous between the aggregated and non-aggregated case, such that every record can be deserialized correctly at the consumer. This requires code not just in the producer, but the consumer as well.</li> </ul>"},{"location":"collection/kinesis_data_streams.html#collection","title":"Collection","text":"<ul> <li>Collection refers to batching multiple Kinesis Data Streams records and sending them in a single HTTP request with a call to the API operation PutRecords, instead of sending each Kinesis Data Streams record in its own HTTP request.</li> </ul> <p>KPL aggregates multiple records into a single Amazon Kinesis Record, and collects multiple Amazon Kinesis Records into a single PutRecords API call.</p>"},{"location":"collection/kinesis_data_streams.html#async-and-futures","title":"Async and Futures","text":"<ul> <li>KPL has an asynchronous interface that does not block.</li> <li>When addUserRecord is called, the record is placed into a queue serviced by another thread and the method returns immediately with a ListenableFuture object.</li> <li>To this, a FutureCallback object can be added that is invoked when the record completes.</li> <li>Adding the callback is optional, which means the API can be used in a fire-and-forget fashion.</li> </ul>"},{"location":"collection/kinesis_data_streams.html#kds-vs-amazon-dynamodb-streams","title":"KDS vs Amazon DynamoDB streams","text":"<ul> <li>Amazon DynamoDB streams offers only 24 hours data retention vs. KDF (configurable upto 365 days)</li> <li>DynamoDB cost is higher than that of KDF</li> </ul>"},{"location":"collection/kinesis_data_streams.html#kinesis-agent","title":"Kinesis Agent","text":"<ul> <li>Kinesis Agent is a stand-alone Java software application that offers an easy way to collect and send data to Kinesis Data Streams. </li> <li>The agent continuously monitors a set of files and sends new data to your stream. </li> <li>The agent handles file rotation, checkpointing, and retry upon failures.</li> <li>By specifying multiple flow configuration settings, you can configure the agent to monitor multiple file directories and send data to multiple streams.</li> <li>Kinesis Agent can send data to KDF (i.e. a delivery stream as well)</li> </ul>"},{"location":"collection/kinesis_data_streams.html#kinesis-data-streams-api","title":"Kinesis Data Streams API","text":"<ul> <li>If your application cannot use the KPL because it cannot incur any additional processing delay, consider using the Kinesis Data Streams API directly.</li> <li>Each PutRecords request can support up to 500 records. </li> <li>Each record in the request can be as large as 1 MB, up to a limit of 5 MB for the entire request, including partition keys.</li> <li>A PutRecords request can include records with different partition keys. The scope of the request is a stream; each request may include any combination of partition keys and records up to the request limits.</li> </ul> <p>Prefer the PutRecords operation described in Adding Multiple Records with PutRecords unless your application specifically needs to always send single records per request, or some other reason PutRecords can't be used.</p> <p>The Kinesis Producer Library takes a lot of work out of frequently occurring use cases. In return, however, you also lose some freedom: many aspects can no longer be controlled independently. In such scenarios, you can use the AWS SDK or the Kinesis Data Streams API.</p>"},{"location":"collection/kinesis_data_streams.html#integrating-with-aws-glue-schema-registry","title":"Integrating with AWS Glue Schema Registry","text":"<ul> <li>Currently, Kinesis Data Streams and AWS Glue schema registry integration is only supported for the Kinesis data streams that use KPL producers implemented in Java. Multi-language support is not provided.</li> </ul>"},{"location":"collection/kinesis_data_streams.html#kinesis-consumer-library-kcl","title":"Kinesis Consumer Library (KCL)","text":"<ul> <li>KCL takes care of many of the complex tasks associated with distributed computing</li> <li>This includes the following:<ul> <li>Load balancing across multiple consumer application instances</li> <li>Responding to consumer application instance failures</li> <li>Checkpointing processed records, and </li> <li>Reacting to resharding</li> </ul> </li> </ul>"},{"location":"collection/kinesis_data_streams.html#kcl-vs-data-streams-api","title":"KCL vs. Data Streams API","text":"<p>The KCL is different from the Kinesis Data Streams APIs that are available in the AWS SDKs. The Kinesis Data Streams APIs help you manage many aspects of Kinesis Data Streams, including creating streams, resharding, and putting and getting records.  The KCL provides a layer of abstraction around all these subtasks, specifically so that you can focus on your consumer application\u2019s custom data processing logic.</p>"},{"location":"collection/kinesis_data_streams.html#kcl-sub-tasks","title":"KCL Sub Tasks","text":"<ul> <li>Connects to the data stream</li> <li>Enumerates the shards within the data stream</li> <li>Uses leases to coordinates shard associations with its workers</li> <li>Instantiates a record processor for every shard it manages</li> <li>Pulls data records from the data stream</li> <li>Pushes the records to the corresponding record processor</li> <li>Checkpoints processed records</li> <li>Balances shard-worker associations (leases) when the worker instance count changes or when the data stream is resharded (shards are split or merged)</li> </ul>"},{"location":"collection/kinesis_data_streams.html#kcl-architecture","title":"KCL Architecture","text":"<p>Key bottleneck areas: - <code>ProvisionedThroughputExceededException</code> for Kinesis Data Stream in CloudWatch Metrics - KCL EC2 Instances CPU Utilization exceeds a certain percentage (say 80%) indicates that EC2 Instances must be auto-scaled. - DynamoDB Write Capacity Unit (WCU) could be low, introducing overall latency.</p> <p></p>"},{"location":"collection/kinesis_data_streams.html#resilience","title":"Resilience","text":"<ul> <li>A record processor could fail. <ul> <li>This is handled by KCL. </li> <li>If you find that your application is throttled consistently, you should consider increasing the number of shards for the stream.</li> </ul> </li> <li>A worker could fail, or the instance of the application that instantiated the worker could fail.<ul> <li>When the application starts up, it instantiates a new worker, which in turn instantiates new record processors that are automatically assigned shards to process. </li> <li>These could be the same shards that these record processors were processing before the failure, or shards that are new to these processors.</li> </ul> </li> <li>An EC2 instance that is hosting one or more instances of the application could fail.<ul> <li>We recommend that you run the EC2 instances for your application in an Auto Scaling group. </li> <li>This way, if one of the EC2 instances fails, the Auto Scaling group automatically launches a new instance to replace it. </li> <li>You should configure the instances to launch your Amazon Kinesis Data Streams application at startup.</li> </ul> </li> <li>Handling duplicate records.<ul> <li>Can occur due to producer retry (or) consumer retry.</li> <li>Your application must anticipate and appropriately handle processing individual records multiple times.</li> <li>Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. </li> <li>Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries.</li> </ul> </li> </ul> <p>If the destination of the final data can handle duplicates well, we recommend relying on the final destination to achieve idempotent processing. For example, with Opensearch you can use a combination of versioning and unique IDs to prevent duplicated processing.</p>"},{"location":"collection/kinesis_data_streams.html#scalability","title":"Scalability","text":"<ul> <li>Scalable by shards. With On-demand mode, number of shards is increased dynamically.</li> <li>Data is replicated across three Availability Zones.</li> </ul>"},{"location":"collection/kinesis_data_streams.html#randon-shard-allocation-vs-specific-shard-allocation","title":"Randon Shard Allocation vs. Specific Shard Allocation","text":""},{"location":"collection/kinesis_data_streams.html#random-partition-keys","title":"Random Partition Keys","text":"<ul> <li>If your use cases do not require data stored in a shard to have high affinity, you can achieve high overall throughput by using a random partition key to distribute data. </li> <li>Random partition keys help distribute the incoming data records evenly across all the shards in the stream and reduce the likelihood of one or more shards getting hit with a disproportionate number of records. </li> <li>You can use a universally unique identifier (UUID) as a partition key to achieve this uniform distribution of records across shards. </li> </ul> <p>This strategy can increase the latency of record processing if the consumer application has to aggregate data from multiple shards. For example, in a leaderboard update scenario (for a gaming application), it is better of each player's data is in a shard (i.e. keep the shard id as player id.)</p>"},{"location":"collection/kinesis_data_streams.html#specific-partition-keys","title":"Specific Partition Keys","text":"<ul> <li>Certain use cases require you to partition data based on specific criteria for efficient processing by the consuming applications. </li> <li>As an example, if you use player ID pk1234 as the hash key, all scores related to that player route to shard1. </li> <li>The consuming application can use the fact that data stored in shard1 has an affinity with the player ID and can efficiently calculate the leaderboard. </li> <li>An increase in traffic related to players mapped to shard1 can lead to a hot shard. </li> <li>Kinesis Data Streams allows you to handle such scenarios by splitting or merging shards without disrupting your streaming pipeline.</li> <li>User activity dashboard based on clickstream for a user can be handled in the same manner (with Session ID as a partition key).<ul> <li>Partitioning by the session ID will allow a single processor to process all the actions for a user session in order.</li> <li>An AWS Lambda function can call the UpdateShardCount API action to change the number of shards in the stream. </li> <li>The KCL will automatically manage the number of processors to match the number of shards.</li> <li>Amazon EC2 Auto Scaling will assure the correct number of instances are running to meet the processing load.</li> </ul> </li> </ul>"},{"location":"collection/kinesis_data_streams.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"collection/kinesis_data_streams.html#producers","title":"Producers","text":"<ul> <li>Producer Application is Writing at a Slower Rate Than Expected<ul> <li>Service Limits Exceeded</li> <li>Producer optimization</li> </ul> </li> <li>Producer Optimization<ul> <li>Large Producer: Either batch &amp; aggregate (KPL or PutRecords or aggregate before PutRecord)</li> <li>Multiple threads</li> <li>Small Producer: Use <code>PutRecords</code></li> <li>Mobile apps must handle intermittent connections inherently and need some sort of batch put, such as PutRecords.</li> </ul> </li> <li>Unauthorized KMS master key permission error</li> </ul>"},{"location":"collection/kinesis_data_streams.html#consumers","title":"Consumers","text":"<ul> <li>The Kinesis Client Library (KCL) relies on your processRecords code to handle any exceptions that arise from processing the data records. Any exception thrown from processRecords is absorbed by the KCL.</li> <li>For any running Kinesis Client Library (KCL) application, a shard only has one owner. However, multiple record processors may temporarily process the same shard. In the case of a worker instance that loses network connectivity, the KCL assumes that the unreachable worker is no longer processing records, after the failover time expires, and directs other worker instances to take over. For a brief period, new record processors and record processors from the unreachable worker may process data from the same shard.</li> <li>Consumer Application is Reading at a Slower Rate Than Expected.</li> <li>GetRecords Returns Empty Records Array Even When There is Data in the Stream<ul> <li>Consuming is a pull model. </li> <li>An empty Records element is returned under two conditions:<ul> <li>There is no more data currently in the shard.</li> <li>There is no data near the part of the shard pointed to by the ShardIterator.</li> <li>The latter condition is subtle, but is a necessary design tradeoff to avoid unbounded seek time (latency) when retrieving records. Thus, the stream-consuming application should loop and call GetRecords, handling empty records as a matter of course.</li> </ul> </li> </ul> </li> <li>Unauthorized KMS master key permission error</li> </ul>"},{"location":"collection/kinesis_data_streams.html#anti-patterns","title":"Anti-patterns","text":"<ul> <li>Small scale consistent throughput \u2013 Even though Kinesis Data Streams works for streaming data at 200 KB/sec or less, KDS is designed and optimized for larger data throughputs.</li> <li>Long-term data storage and analytics: Kinesis Data Streams is not suited for long-term data storage. By default, data is retained for 24 hours, and you can extend the retention period by up to 7 days. You can move any data that needs to be stored for longer than 7 days into another durable storage service such as Amazon S3, Amazon S3 Glacier, Amazon Redshift, or DynamoDB.</li> </ul>"},{"location":"collection/kinesis_data_streams.html#kinesis-vs-msk","title":"Kinesis vs. MSK","text":"<ul> <li>MSK provides possibilities of sending large messages (ex: 10MB) into Kafka after custom configuration</li> <li>On the other hand, on a Kinesis shard, only 1 MB/second can be ingested</li> </ul>"},{"location":"collection/kinesis_data_streams.html#msk","title":"MSK","text":"<ul> <li>Authentication &amp; Authorization (important):<ul> <li>Define who can read/write to which topics</li> <li>Mutual TLS (AuthN) + Kafka ACLs (AuthZ)</li> <li>SASL/SCRAM (AuthN) + Kafka ACLs (AuthZ)</li> <li>IAM Access Control (AuthN + AuthZ) - recommended.</li> </ul> </li> <li>Use MSK Connect (Managed) to pull data from MSK Cluster and push into S3</li> <li>You can deploy any Kafka Connect connectors to MSK Connect as a plugin<ul> <li>Amazon S3, Amazon Redshift, Amazon OpenSearch, Debezium, etc\u2026 </li> </ul> </li> </ul>"},{"location":"collection/kinesis_data_streams.html#references","title":"References","text":"<ul> <li>Kinesis vs Kafka</li> <li>Feeding and eating Kinesis Streams with Python</li> </ul>"},{"location":"exam_prep/exam_preparation.html","title":"Exam Preparation","text":""},{"location":"exam_prep/exam_preparation.html#domain-level-breakup","title":"Domain Level Breakup","text":"Domain Percentage Collection 18% Storage and Data Management 22% Processing 24% Analysis and Visualization 18% Security 18%"},{"location":"exam_prep/exam_preparation.html#cognitive-level-of-questions","title":"Cognitive level of questions","text":"<p>Specialty-level exam questions are not designed to solely test your recall of an AWS service. Instead, they challenge you to apply your knowledge of AWS services, features, and related concepts to a given scenario.</p>"},{"location":"exam_prep/exam_preparation.html#strategies","title":"Strategies","text":"<ol> <li>Read and Understand the question first.</li> <li>Identify the key phrases and qualifiers.</li> <li>Try to answer the question before even looking at the answer choices.</li> <li>Eliminate answer options based on key phrases and qualifiers.</li> <li>_Flag &amp; move on as required. But remember to answer all the questions).</li> </ol>"},{"location":"exam_prep/exam_preparation.html#question-components","title":"Question Components","text":"<p>AWS specialty exam questions contain two components: scenario and question.</p> <p>Scenario: This portion frames the situation on which the question is based. It lists services, problems, and requirements that will help you come to a correct response. You will always have all the information you need to identify the correct response. Do not try to add to or remove information from the scenario.</p> <p>Question: The question always specifies the exact conditions that must be met. If more than one response is required, the question always specifies the number of required responses. If nothing is specified, there is only one correct response.</p>"},{"location":"processing/amazon_emr.html","title":"Amazon EMR","text":"<ul> <li>Amazon EMR is highly scalable big data platform that supports open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi (Incubating), and Presto.</li> <li>With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark.</li> <li>For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used.</li> <li>For long-running workloads, you can create highly available clusters that automatically scale to meet demand.</li> </ul> <p>Amazon EMR supports a number of databases as target </p>"},{"location":"processing/amazon_emr.html#apache-ranger-on-emr","title":"Apache Ranger on EMR","text":"<p>Apache Ranger is a role-based access control framework to enable, monitor, and manage comprehensive data security across the Hadoop platform. </p>"},{"location":"processing/amazon_emr.html#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Centralized security administration and auditing.</li> <li>Fine-grained authorization across many Hadoop components (Hadoop, Hive, HBase, Storm, Knox, Solr, Kafka, and YARN).</li> <li>Syncs policies and users by using agents and plugins that run within the same process as the Hadoop component.</li> <li>Supports row-level authentication and auditing capabilities with embedded search.</li> </ul>"},{"location":"processing/amazon_emr.html#amazon-emr-events","title":"Amazon EMR &amp; Events","text":"<ul> <li>You can now respond to Amazon EMR cluster state changes with Amazon CloudWatch Events. </li> <li>The new Amazon EMR event types in Amazon CloudWatch Events provide information including state and related severity for Amazon EMR clusters, instance groups, steps, and Auto Scaling policies.</li> </ul>"},{"location":"processing/amazon_emr.html#emrfs","title":"EMRFS","text":"<ul> <li>EMRFS uses an Amazon DynamoDB database to store object metadata and track consistency in Amazon S3.</li> <li>EMRFS allows you to define retry rules for processing inconsistencies.</li> </ul>"},{"location":"processing/amazon_emr.html#s3distcp-tool","title":"S3DistCP Tool","text":"<ul> <li>Apache DistCp is an open-source tool you can use to copy large amounts of data. </li> <li>S3DistCp is similar to DistCp, but optimized to work with AWS, particularly Amazon S3. S3DistCp is more performant than DistCp. </li> <li>The command for S3DistCp in Amazon EMR version 4.0 and later is s3-dist-cp, which you add as a step in a cluster or at the command line. </li> <li>Using S3DistCp, you can efficiently copy large amounts of data from Amazon S3 into HDFS where it can be processed by subsequent steps in your Amazon EMR cluster. </li> <li>You can also use S3DistCp to copy data between Amazon S3 buckets or from HDFS to Amazon S3. </li> <li>S3DistCp is more scalable and efficient for parallel copying large numbers of objects across buckets and across AWS accounts.</li> </ul> <p>By adding S3DistCp as a step in a job flow, you can efficiently copy large amounts of data from Amazon S3 into HDFS, where subsequent steps in your EMR clusters can process it.</p> <p>S3DistCp does not support concatenation for Parquet files. When you attempt to concatenate them, you get an error message like this: \"Expected n values in column chunk at /path/to/concatenated/parquet/file offset m but got x values instead over y pages ending at file offset z\" Amazon recommends using PySpark instead. You can't specify the target file size in PySpark, but you can specify the number of partitions. Spark saves each partition to a separate output file. To estimate the number of partitions that you need, divide the size of the dataset by the target individual file size.</p> <p></p>"},{"location":"processing/amazon_emr.html#emrfs-identity-access-management-iam","title":"EMRFS Identity &amp; Access Management (IAM)","text":"<ul> <li>EMRFS uses the permissions attached to the service role for cluster EC2 instances by default.</li> <li>When you have multiple cluster users and multiple data stores, you may want users to have different permissions to EMRFS data in Amazon S3. </li> <li>To do this, you can you can use IAM roles for EMRFS. </li> <li>This allows EMRFS to assume different roles with different permissions policies based on the user or group making the request or the location of EMRFS data in Amazon S3.</li> </ul> <p>Because IAM roles for EMRFS will fall back to the permissions attached to the service role for cluster EC2 instances, as a best practice, we recommend that you use IAM roles for EMRFS, and limit the EMRFS and Amazon S3 permissions attached to the service role for cluster EC2 instances.</p> <p>With the release of Amazon S3 strong read-after-write consistency on December 1, 2020, you no longer need to use EMRFS consistent view (EMRFS CV) with your Amazon EMR clusters.</p>"},{"location":"processing/amazon_emr.html#notifications","title":"Notifications","text":"<p>Amazon EMR can only be configured as a publisher to an SNS topic\u2014it cannot subscribe to notifications.</p>"},{"location":"processing/amazon_emr.html#high-availability","title":"High Availability","text":"<ul> <li>Amazon EMR supports multiple master nodes to enable high availability for EMR applications. </li> <li>Launch an EMR cluster with three master nodes and support high availability applications like YARN Resource Manager, HDFS Name Node, Spark, Hive, and Ganglia. </li> <li>EMR clusters with multiple master nodes are not tolerant of Availability Zone failures. In the case of an Availability Zone outage, you lose access to the EMR cluster.</li> <li>Using the Amazon EMR version 5.7.0 or later, you can set up a read-replica cluster, which allows you to maintain read-only copies of data in Amazon S3. In the event that the primary cluster becomes unavailable, you can access the data from the read-replica cluster to perform read operations simultaneously.</li> </ul>"},{"location":"processing/amazon_emr.html#auto-scaling","title":"Auto-scaling","text":"<p>When you create a cluster and specify the configuration of the master node, core nodes, and task nodes, you have two configuration options. You can use:</p> <ul> <li>Instance fleets</li> <li>Instance groups (provides autoscaling)</li> </ul> <p>When automatic scaling is configured, Amazon EMR adds and removes instances based on Amazon CloudWatch metrics that you specify.</p> <p>The following are two of the most common metrics used for automatic scaling in Amazon EMR:</p> <ul> <li>YarnMemoryAvailablePercentage: The percentage of remaining memory available to YARN.</li> <li>ContainerPendingRatio: The ratio of pending containers to containers allocated. You can use this metric to scale a cluster based on container-allocation behavior for varied loads, which is useful for performance tuning.</li> <li>CapacityRemainingGB metric is the amount of remaining HDFS disk capacity.</li> </ul>"},{"location":"processing/amazon_emr.html#metastore","title":"Metastore","text":"<p>Using Amazon EMR version 5.8.0 or later, you can configure Hive to use the AWS Glue Data Catalog as its metastore. This configuration is recommended if the metastore is shared by different clusters.</p>"},{"location":"processing/amazon_emr.html#emr-components","title":"EMR Components","text":""},{"location":"processing/amazon_emr.html#presto","title":"Presto","text":"<ul> <li>Presto is a fast SQL query engine designed for interactive analytic queries over large datasets from multiple sources. </li> <li>It supports both non-relational sources, such as the Hadoop Distributed File System (HDFS), Amazon S3, and HBase, and relational data sources such as MySQL, PostgreSQL, and Amazon Redshift. </li> <li>Presto can query data where it\u2019s stored, without needing to move data into a separate analytics system. </li> <li>Query execution runs in parallel over a pure memory-based architecture, with most results returning in seconds.</li> </ul> <p>You can launch a Presto on Amazon EMR cluster in minutes. You don\u2019t need to worry about node provisioning, cluster setup, configuration, or cluster tuning. Amazon EMR takes care of these tasks so you can focus on analysis.</p> <p>If the use case is Interactive Analytic queries over large datasets from multiple sources, choose Presto on EMR.</p>"},{"location":"processing/amazon_emr.html#spark","title":"Spark","text":"<ul> <li>Apache Spark is an open-source, distributed processing system used for big data workloads. </li> <li>It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. </li> <li>It provides development APIs in Java, Scala, Python and R, and supports code reuse across multiple workloads\u2014batch processing, interactive queries, real-time analytics, machine learning, and graph processing.</li> <li>One of the speed advantages of Apache Spark comes from loading data into immutable dataframes, which can be accessed repeatedly in memory. </li> <li>Spark DataFrames organizes distributed data into columns. This makes summaries and aggregates much quicker to calculate.</li> </ul>"},{"location":"processing/amazon_emr.html#stream-processing","title":"Stream processing","text":"<ul> <li>Consume and process real-time data from Amazon Kinesis, Apache Kafka, or other data streams with Spark Streaming on EMR. </li> <li>Perform streaming analytics in a fault-tolerant way and write results to S3 or on-cluster HDFS.</li> </ul> <p>For a more efficient stream processing Flink could be a better fit than Spark (since Spark employs a micro-batch approach)</p>"},{"location":"processing/amazon_emr.html#interactive-sql","title":"Interactive SQL","text":"<ul> <li>Use Spark SQL for low-latency, interactive queries with SQL or HiveQL. </li> <li>Spark on EMR can leverage EMRFS, so you can have ad hoc access to your datasets in S3. </li> <li>Also, you can utilize EMR Studio, EMR Notebooks, Zeppelin notebooks, or BI tools via ODBC and JDBC connections.</li> </ul>"},{"location":"processing/amazon_emr.html#hive","title":"Hive","text":"<p>Apache Hive is an open-source, distributed, fault-tolerant system that provides data warehouse-like query capabilities. It enables users to read, write, and manage petabytes of data using a SQL-like interface.</p> <p>Hive vs HBase: Apache HBase is a NoSQL distributed database that enables random, strictly consistent, real-time access to petabytes of data. Apache Hive is a distributed data warehouse system that provides SQL-like querying capabilities.</p>"},{"location":"processing/amazon_emr.html#ad-hoc-sql-queries-on-s3","title":"Ad-Hoc SQL Queries on S3","text":"<ul> <li>Airbnb uses Amazon EMR to run Apache Hive on a S3 data lake. </li> <li>Running Hive on the EMR clusters enables Airbnb analysts to perform ad hoc SQL queries on data stored in the S3 data lake. </li> <li>By migrating to a S3 data lake, Airbnb reduced expenses, can now do cost attribution, and increased the speed of Apache Spark jobs by three times their original speed.</li> </ul> <p>Hive enables analysts to perform ad hoc SQL queries on data stored in the S3 data lake. This reduces operational costs.</p> <p>Using Amazon EMR version 5.8.0 or later, you can configure Hive to use the AWS Glue Data Catalog as its metastore. This configuration is recommended if the metastore is shared by different clusters.</p> <p>Hive vs. Impala: The main difference between Hive and Impala is that the Hive is a data warehouse software that can be used to access and manage large distributed datasets built on Hadoop while Impala is a massive parallel processing SQL engine for managing and analyzing data stored on Hadoop.</p>"},{"location":"processing/amazon_emr.html#hbase","title":"HBase","text":"<ul> <li>Apache HBase is an open-source, NoSQL, distributed big data store. </li> <li>It enables random, strictly consistent, real-time access to petabytes of data. </li> <li>HBase is very effective for handling large, sparse datasets.</li> <li>HBase integrates seamlessly with Apache Hadoop and the Hadoop ecosystem and runs on top of the Hadoop Distributed File System (HDFS) or Amazon S3 using Amazon Elastic MapReduce (EMR) file system, or EMRFS. </li> <li>HBase serves as a direct input and output to the Apache MapReduce framework for Hadoop, and works with Apache Phoenix to enable SQL-like queries over HBase tables.</li> </ul>"},{"location":"processing/amazon_emr.html#hudi","title":"Hudi","text":"<p>Hudi brings (a) transactions (b) change streams and (c) record-level updates/deletes to Data Lakes.</p> <p></p>"},{"location":"processing/amazon_emr.html#transient-clusters","title":"Transient Clusters","text":"<ul> <li>Amazon EMR can be used to quickly and cost-effectively perform data transformation workloads (ETL) such as sort, aggregate, and join on large datasets. </li> <li>You can use a transient cluster to aggregate the sensors' data each night. After it is completed, the transient clusters are automatically terminated. </li> <li>This service will help you save costs since it will only run each night, and you can use Amazon QuickSight to get insights instantly and effortlessly.</li> </ul>"},{"location":"processing/amazon_emr.html#process-data-with-custom-jar","title":"Process Data with Custom JAR","text":"<ul> <li>A custom JAR runs a compiled Java program that you can upload to Amazon S3. </li> <li>You should compile the program against the version of Hadoop you want to launch, and submit a CUSTOM_JAR step to your Amazon EMR cluster.</li> </ul>"},{"location":"processing/amazon_emr.html#encryption","title":"Encryption","text":"<ul> <li>You can choose to encrypt data in Amazon S3, local disks, or both.</li> <li>When you create a security configuration, you specify two sets of encryption options: at-rest data encryption and in-transit data encryption. </li> <li>Options for at-rest data encryption include both Amazon S3 with EMRFS and local-disk encryption. </li> <li>In-transit encryption options enable the open-source encryption features for certain applications that support Transport Layer Security (TLS). </li> <li>At-rest options and in-transit options can be enabled together or separately.</li> </ul>"},{"location":"processing/amazon_emr.html#emr-and-dynamodb","title":"EMR and DynamoDB","text":"<ul> <li>AWS offers out-of-the-box Amazon Elastic MapReduce (Amazon EMR) integration with Amazon DynamoDB, providing customers an integrated solution that eliminates the often prohibitive costs of administration, maintenance, and upfront hardware.</li> </ul>"},{"location":"processing/athena.html","title":"Athena","text":"<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p>"},{"location":"processing/athena.html#best-practices","title":"Best Practices","text":"<ul> <li>Partition your data</li> <li>Bucket your data</li> <li>Use compression</li> <li>Optimize file size</li> <li>Optimize columnar data store generation</li> </ul>"},{"location":"processing/athena.html#cost-control","title":"Cost Control","text":"<p>Amazon Athena allows you to set two types of cost controls: - per-query limit - per-workgroup limit (a.k.a workgroup-wide data usage control limit)</p> <ul> <li>The per-query control limit specifies the total amount of data scanned per query. If any query that runs in the workgroup exceeds the limit, it is canceled.</li> <li>Use workgroups to separate users, teams, applications, or workloads, to set limits on the amount of data that each query or the entire workgroup can process, and to track costs. </li> <li>Because workgroups act as resources, you can use resource-level identity-based policies to control access to a specific workgroup. </li> <li>You can also view query-related metrics in Amazon CloudWatch, control costs by configuring limits on the amount of data scanned, and create thresholds and trigger actions, such as Amazon SNS, when these thresholds are breached.</li> </ul>"},{"location":"processing/athena.html#references","title":"References","text":"<ul> <li>Create Tables in Athena from Nested JSON and mappings using JSON Serde</li> </ul>"},{"location":"processing/athena.html#athena-vs-elasticsearch","title":"Athena vs. ElasticSearch","text":"<ul> <li>Amazon Elasticsearch Service can be relatively more expensive as you have to run an EC2 instance that charges you per-hour. </li> <li>Amazon Athena is more suitable for ad-hoc analysis.</li> </ul>"},{"location":"processing/aws_iot.html","title":"AWS IOT","text":"<ul> <li>AWS IoT provides the cloud services that connect your IoT devices to other devices and AWS cloud services. </li> <li>AWS IoT provides device software that can help you integrate your IoT devices into AWS IoT-based solutions. </li> <li>If your devices can connect to AWS IoT, AWS IoT can join them in the AWS cloud services.</li> <li>AWS IoT Analytics automates the steps required to analyze data from IoT devices. </li> <li>AWS IoT Analytics filters, transforms, and enriches IoT data before storing it in a time-series data store for analysis.</li> <li>You can set up the service to collect only the data you need from your devices, apply mathematical transforms to process the data, and enrich the data with device-specific metadata such as device type and location before storing it.</li> <li>You can then analyze your data by running queries using the built-in SQL query engine or perform more complex analytics and machine learning inference.</li> </ul>"},{"location":"processing/glue.html","title":"Glue","text":""},{"location":"processing/glue.html#pyspark-extensions","title":"PySpark extensions","text":""},{"location":"processing/glue.html#relationalize","title":"Relationalize","text":"<ul> <li>Converts a DynamicFrame into a form that fits within a relational database.</li> <li>Relationalizing a DynamicFrame is especially useful when you want to move data from a NoSQL environment like DynamoDB into a relational database like MySQL.</li> <li>Example</li> </ul>"},{"location":"processing/glue.html#data-access-security","title":"Data Access &amp; Security","text":"<ul> <li>AWS Glue resource policies can be used to control access to Data Catalog resources.</li> </ul>"},{"location":"processing/glue.html#glue-crawler","title":"Glue Crawler","text":"<ul> <li>AWS Glue can crawl data in different AWS Regions. </li> <li>When you define an Amazon S3 data store to crawl, you can choose whether to crawl a path in your account or another account or another region.</li> <li>Use one or more of the following methods to reduce crawler run times.<ul> <li>Use an exclude pattern</li> <li>Run multiple crawlers</li> <li>Combine smaller files to create larger ones</li> </ul> </li> </ul>"},{"location":"processing/glue.html#glue-data-catalog","title":"Glue Data Catalog","text":""},{"location":"processing/glue.html#glue-support","title":"Glue support","text":"<p>AWS Glue job usually executes Apache Spark, Spark Streaming, or Python shell scripts only. </p> <p>AWS Glue doesn't directly support Apache Hive.</p>"},{"location":"processing/processing.html","title":"Processing","text":""},{"location":"processing/processing.html#glue","title":"Glue","text":"<ul> <li>Serverless ETL that simplifies the process of maintaining and running jobs. AWS Glue is Apache Spark-based.</li> <li>Glue can be used to connect to Athena, Redshift, and Quicksight - as well as being used as a Hive metastore.</li> <li>Hourly rate, billed by the second</li> </ul>"},{"location":"processing/processing.html#emr","title":"EMR","text":"<ul> <li>Provides you with lower-level access to your Hadoop environment and greater flexibility in using tools beyond Spark.</li> <li>Per second + Amazon Elastic Compute Cloud (Amazon EC2). EC2 costs can be optimized by using Spot Instances for Task nodes.</li> </ul>"},{"location":"processing/processing.html#streaming","title":"Streaming","text":"<ul> <li>You can use Spark Streaming on EMR to transform data into a format that is easily storable in ElastiCache for Redis.</li> </ul>"},{"location":"processing/processing.html#emrfs","title":"EMRFS","text":"<ul> <li>EMRFS provides the convenience of storing persistent data in Amazon S3 for use with Hadoop while also providing features like data encryption.</li> <li>EMR allows you use S3 instead of HDFS for HBase\u2019s data via EMRFS.</li> </ul>"},{"location":"processing/processing.html#use-cases-for-emrfs","title":"Use cases for EMRFS","text":"<ul> <li>Storage of HBase StoreFiles and metadata on S3</li> <li>HBase read-replicas on S3</li> <li>Snapshots of HBase data to S3</li> </ul>"},{"location":"processing/processing.html#emr-tools","title":"EMR Tools","text":"<ul> <li>Ganglia is the operational dashboard provided with EMR.</li> <li>Hue and Ambari are graphical front-ends for interacting with a cluster, but only Hue is provided with EMR. </li> <li>Presto is used for querying multiple data stores at once (used by Athena).</li> </ul>"},{"location":"processing/processing.html#ganglia","title":"Ganglia","text":"<ul> <li>The Ganglia open source project is a scalable, distributed system designed to monitor clusters and grids while minimizing the impact on their performance.</li> </ul>"},{"location":"processing/processing.html#encryption","title":"Encryption","text":"<ul> <li>SSE-KMS</li> <li>LUKS (Linux Unified Key Setup) Encryption: Disk encryption specification that can help encrypt sensitive data at rest.</li> <li>KMS Encryption</li> </ul>"},{"location":"processing/processing.html#keep-redshift-and-rds-in-sync","title":"Keep Redshift and RDS in sync","text":"<ul> <li>Materialized View. The disadvantage of the materialized view is that refreshes copy all of the data from the beginning.</li> <li>DB Link functionality to copy data from Redshift to RDS. DB Link is a feature in PostgreSQL that executes query in a remote database.</li> </ul>"},{"location":"processing/processing.html#emr-auto-scaling","title":"EMR Auto-scaling","text":"<ul> <li>The ability to scale the number of nodes in your cluster up and down on the fly is among the major features that make Amazon EMR elastic. </li> <li>You can take advantage of scaling in EMR by resizing your cluster down when you have little or no workload. </li> <li>You can also scale your cluster up to add processing power when the job gets too slow. </li> <li>This allows you to spend just enough to cover the cost of your job and little more.</li> <li>Task nodes only run Hadoop tasks through YARN and DO NOT store data in HDFS. So they get decommissioned fast and hence can scale down faster.</li> <li>HDFS can take relatively longer time to decommission. This is because HDFS block replication is throttled by design through configurations located in hdfs-site.xml.</li> <li>Hence, core nodes scaling down takes longer time.</li> </ul> <p>A common automatic scaling metric for core nodes is HDFSUtilization. Common auto scaling metrics for task nodes include ContainerPendingRatio and YarnMemoryAvailablePercentage. Unless you need the HDFS storage, scaling task nodes is usually a better option, and provides with more processing power.</p> <p>For core nodes, you can think of scale-out policies as easily triggered with a low cooldown and small node increments. Scale-in policies should be hard to trigger, with larger cooldowns and node increments.</p>"},{"location":"processing/processing.html#lambda","title":"Lambda","text":"<ul> <li>Provides real-time stream and file processing and is often used to replace cron jobs.</li> <li>Lambda can also be used in conjuction with Kinesis Data Streams (with event sourcing)</li> </ul>"},{"location":"processing/processing.html#references","title":"References","text":"<ul> <li>Best Practices for securing EMR</li> <li>EMR - What's New?</li> </ul>"},{"location":"security/emr_security.html","title":"EMR Security","text":""},{"location":"security/emr_security.html#encrypting-your-s3-buckets-with-different-encryption-modes-and-keys","title":"Encrypting your S3 buckets with different encryption modes and keys","text":"<ul> <li>With S3 encryption on Amazon EMR, all the encryption modes use a single CMK by default to encrypt objects in S3. </li> <li>If you have highly sensitive content in specific S3 buckets, you may want to manage the encryption of these buckets separately by using different CMKs or encryption modes for individual buckets. </li> <li>You can accomplish this using the per bucket encryption overrides option in Amazon EMR.</li> </ul> <p>With per bucket encryption overrides, you can choose optimal encryption overrides for specific buckets.</p>"},{"location":"security/redshift_security.html","title":"Redshift Security","text":""},{"location":"security/redshift_security.html#s3-encryption","title":"S3 Encryption","text":"<p>The COPY command supports the following types of Amazon S3 encryption:</p> <ul> <li>Server-side encryption with Amazon S3-managed keys (SSE-S3)</li> <li>Server-side encryption with AWS KMS keys (SSE-KMS)</li> <li>Client-side encryption using a client-side symmetric root key</li> </ul> <p>The COPY command doesn't support the following types of Amazon S3 encryption:</p> <ul> <li>Server-side encryption with customer-provided keys (SSE-C)</li> <li>Client-side encryption using an AWS KMS key</li> <li>Client-side encryption using a customer-provided asymmetric root key</li> </ul>"},{"location":"security/security.html","title":"Security","text":""},{"location":"security/security.html#tokenization","title":"Tokenization","text":"<p>Tokenization can be used to add a layer of explicit access controls to de-tokenization of individual data items, which can be used to implement and demonstrate least-privileged access to sensitive data. For instances where data may be co-mingled in a common repository such as a data lake, tokenization can help ensure that only those with the appropriate access can perform the de-tokenization process and reveal sensitive data.</p>"},{"location":"security/security.html#aws-artifact","title":"AWS Artifact","text":"<p>AWS Artifact provides on-demand access to AWS' compliance and security related information. </p>"},{"location":"security/security.html#key-characteristics","title":"Key characteristics:","text":"<ul> <li>Self-service document retrieval portal </li> <li>Review, accept, and manage your agreements with AWS (such as the BAA and NDAs)</li> <li>Share evidence of AWS security controls by providing AWS artifact compliance documents to regulators and auditors </li> </ul>"},{"location":"security/security.html#compliance-with-aws-config","title":"Compliance with AWS Config","text":"<p>For example, you can use an AWS Config rule to monitor the settings of Amazon S3 bucket Access Control Lists (ACLs) and S3 bucket policies to look for public read or public write access violations.</p>"},{"location":"security/security.html#amazon-macie","title":"Amazon Macie","text":"<p>Amazon Macie gives you an automated and low touch way to discover and classify your business data and detect sensitive information such as personally identifiable information (PII) and credential data.</p>"},{"location":"storage/redshift.html","title":"Redshift","text":""},{"location":"storage/redshift.html#best-practices","title":"Best Practices","text":""},{"location":"storage/redshift.html#copy-data-from-multiple-evenly-sized-files","title":"COPY Data from multiple, evenly sized files","text":"<ul> <li>When loading multiple files into a single table, use a single COPY command for the table, rather than multiple COPY commands.</li> <li>Amazon Redshift automatically parallelizes the data ingestion. </li> <li>Using a single COPY command to bulk load data into a table ensures optimal use of cluster resources, and quickest possible throughput.</li> <li>A COPY command is the most efficient way to load a table. </li> <li>You can also add data to your tables using INSERT commands, though it is much less efficient than using COPY. </li> <li>The COPY command is able to read from multiple data files or multiple data streams simultaneously. </li> <li>Amazon Redshift allocates the workload to the cluster nodes and performs the load operations in parallel, including sorting the rows and distributing data across node slices.</li> </ul> <p>The COPY command leverages the Amazon Redshift massively parallel processing (MPP) architecture to read and load data in parallel from files in an Amazon S3 bucket.  You can take maximum advantage of parallel processing by splitting your data into multiple files and setting distribution keys on your tables.</p> <p>If you use multiple concurrent COPY commands to load one table from numerous files, Amazon Redshift is forced to perform a serialized load. This type of load is much slower and requires a VACUUM process at the end in the case that the table has a sort column defined. So, use a single COPY command that allows Redshift to employ MPP to read &amp; load data in parallel from files in an Amazon S3 Bucket.</p> <p>Furthermore, AWS recommends that you efficiently update and insert new data and load data into a staging table first. Use temporary staging tables to hold the data for transformation. These tables are automatically dropped after the ETL session is complete. Temporary tables can be created using the CREATE TEMPORARY TABLE syntax, or by issuing a SELECT \u2026 INTO #TEMP_TABLE query. Explicitly specifying the CREATE TEMPORARY TABLE statement allows you to control the DISTRIBUTION KEY, SORT KEY, and compression settings to improve performance further.</p> <p>You can use a manifest to ensure that the COPY command loads all of the required files, and only the required files, for a data load. This can also be used to load files from different buckets or files that do not share the same prefix. Instead of supplying an object path for the COPY command, you supply the name of a JSON-formatted text file that explicitly lists the files to be loaded. The URL in the manifest must specify the bucket name and full object path for the file, not just a prefix.</p>"},{"location":"storage/redshift.html#use-wlm-workload-management","title":"Use WLM (Workload Management)","text":"<p>Use Amazon Redshift\u2019s workload management (WLM) to define multiple queues dedicated to different workloads (for example, ETL versus reporting) and to manage the runtimes of queries. As you migrate more workloads into Amazon Redshift, your ETL runtimes can become inconsistent if WLM is not appropriately set up.</p>"},{"location":"storage/redshift.html#perform-table-maintenance-regularly","title":"Perform Table Maintenance regularly","text":"<p>Amazon Redshift is a columnar database, which enables fast transformations for aggregating data. Performing regular table maintenance ensures that transformation ETLs are predictable and performant. To get the best performance from your Amazon Redshift database, you must ensure that database tables regularly are VACUUMed and ANALYZEd.</p>"},{"location":"storage/redshift.html#perform-multiple-steps-in-a-single-transaction","title":"Perform multiple steps in a single transaction","text":"<p>ETL transformation logic often spans multiple steps. Because commits in Amazon Redshift are expensive, if each ETL step performs a commit, multiple concurrent ETL processes can take a long time to execute.</p>"},{"location":"storage/redshift.html#loading-data-in-bulk","title":"Loading Data in Bulk","text":"<p>Amazon Redshift is designed to store and query petabyte-scale datasets. Using Amazon S3 you can stage and accumulate data from multiple source systems before executing a bulk COPY operation. </p> <p>The following methods allow efficient and fast transfer of these bulk datasets into Amazon Redshift:</p> <ul> <li>Use a manifest file to ingest large datasets that span multiple files. The manifest file is a JSON file that lists all the files to be loaded into Amazon Redshift. Using a manifest file ensures that Amazon Redshift has a consistent view of the data to be loaded from S3, while also ensuring that duplicate files do not result in the same data being loaded more than one time.</li> <li>Use temporary staging tables to hold the data for transformation. These tables are automatically dropped after the ETL session is complete. Temporary tables can be created using the CREATE TEMPORARY TABLE syntax, or by issuing a SELECT \u2026 INTO #TEMP_TABLE query. Explicitly specifying the CREATE TEMPORARY TABLE statement allows you to control the DISTRIBUTION KEY, SORT KEY, and compression settings to further improve performance.</li> <li>Use ALTER table APPEND to swap data from the staging tables to the target table. Data in the source table is moved to matching columns in the target table. Column order doesn\u2019t matter. After data is successfully appended to the target table, the source table is empty. ALTER TABLE APPEND is much faster than a similar CREATE TABLE AS or INSERT INTO operation because it doesn\u2019t involve copying or moving data.</li> </ul>"},{"location":"storage/redshift.html#use-redshift-spectrum-for-ad-hoc-etl-processing","title":"Use Redshift Spectrum for ad hoc ETL processing","text":"<ul> <li>When you partition your data, you can restrict the amount of data that Redshift Spectrum scans by filtering on the partition key. </li> <li>You can partition your data by any key. A common practice is to partition the data based on time.</li> </ul>"},{"location":"storage/redshift.html#backups","title":"Backups","text":"<ul> <li>To create a point-in-time backup of a cluster, you can use a snapshot. </li> <li>There are two types of snapshots: automated and manual. </li> <li>The backups you will create are stored in an Amazon S3 bucket. </li> </ul>"},{"location":"storage/redshift.html#automated-snapshot","title":"Automated Snapshot","text":"<ul> <li>If an automated snapshot is enabled, Amazon Redshift will take a snapshot every eight hours or following every 5 GB per node of data changes, or whichever comes first.</li> </ul>"},{"location":"storage/redshift.html#manual-snapshot","title":"Manual Snapshot","text":"<ul> <li>A manual snapshot can be taken at any time.</li> <li>By default, manual snapshots are retained indefinitely, even after you delete your cluster.</li> </ul>"},{"location":"storage/redshift.html#best-practices_1","title":"Best Practices","text":"<ul> <li>Distribute the fact table and one dimension table on their common columns.<ul> <li>Your fact table can have only one distribution key.</li> <li>Any tables that join on another key aren't collocated with the fact table.</li> <li>Choose one dimension to collocate based on how frequently it is joined and the size of the joining rows.</li> <li>Designate both the dimension table's primary key and the fact table's corresponding foreign key as the DISTKEY.</li> </ul> </li> <li>Choose a column with high cardinality in the filtered result set.<ul> <li>If you distribute a sales table on a date column, for example, you should probably get fairly even data distribution, unless most of your sales are seasonal.</li> <li>However, if you commonly use a range-restricted predicate to filter for a narrow date period, most of the filtered rows occur on a limited set of slices and the query workload is skewed.</li> </ul> </li> <li>Choose the largest dimension based on the size of the filtered dataset.</li> <li>Change some dimension tables to use ALL distribution.</li> </ul>"},{"location":"storage/redshift.html#table-distribution-style","title":"Table Distribution Style","text":"<p>A table might be defined with a DISTSTYLE of EVEN, KEY, or ALL.</p> <ul> <li>EVEN will do a round-robin distribution of data.</li> <li>KEY requires a single column to be defined as a DISTKEY. On ingest, Amazon Redshift hashes each DISTKEY column value, and route hashes to the same slice consistently.</li> <li>ALL distribution stores a full copy of the table on the first slice of each node.</li> </ul> <p>Which style is most appropriate for your table is determined by several criteria. </p>"},{"location":"storage/s3.html","title":"S3","text":""},{"location":"storage/s3.html#s3-prefix","title":"S3 Prefix","text":"<ul> <li>You can use prefixes to organize the data that you store in Amazon S3 buckets. </li> <li>A prefix is a string of characters at the beginning of the object key name. </li> <li>A prefix can be any length, subject to the maximum length of the object key name (1,024 bytes).</li> <li>You can think of prefixes as a way to organize your data in a similar way to directories. However, prefixes are not directories.</li> </ul> <p>Searching by prefix limits the results to only those keys that begin with the specified prefix. The delimiter causes a list operation to roll up all the keys that share a common prefix into a single summary list result.</p> <p>The purpose of the prefix and delimiter parameters is to help you organize and then browse your keys hierarchically. </p> <p>To do this, - first pick a delimiter for your bucket, such as slash (/), that doesn't occur in any of your anticipated key names.  - You can use another character as a delimiter. There is nothing unique about the slash (/) character, but it is a very common prefix delimiter. - Next, construct your key names by concatenating all containing levels of the hierarchy, separating each level with the delimiter.</p>"},{"location":"storage/s3.html#example","title":"Example","text":"<p>For example, if you were storing information about cities, you might naturally organize them by continent, then by country, then by province or state. Because these names don't usually contain punctuation, you might use slash (/) as the delimiter. The following examples use a slash (/) delimiter.</p> <ul> <li>Europe/France/Nouvelle-Aquitaine/Bordeaux</li> <li>North America/Canada/Quebec/Montreal</li> <li>North America/USA/Washington/Bellevue</li> <li>North America/USA/Washington/Seattle</li> </ul>"},{"location":"storage/s3.html#performance","title":"Performance","text":"<ul> <li>Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3.</li> <li>Amazon S3 automatically scales to high request rates. </li> <li>For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per partitioned Amazon S3 prefix. </li> <li>There are no limits to the number of prefixes in a bucket. </li> <li>You can increase your read or write performance by using parallelization. </li> <li>For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55, 000 read requests per second. </li> <li>Similarly, you can scale write operations by writing to multiple prefixes. The scaling, in the case of both read and write operations, happens gradually and is not instantaneous. </li> <li>While Amazon S3 is scaling to your new higher request rate, you may see some 503 (Slow Down) errors. </li> <li>These errors will dissipate when the scaling is complete.</li> </ul> <p>Some data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data. These data lake applications achieve single-instance transfer rates that maximize the network interface use for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance. These applications then aggregate throughput across multiple instances to get multiple terabits per second.</p> <p>Other applications are sensitive to latency, such as social media messaging applications. These applications can achieve consistent small object latencies (and first-byte-out latencies for larger objects) of roughly 100\u2013200 milliseconds.</p>"},{"location":"storage/storage.html","title":"Storage","text":"<ul> <li>Operational Data Store (OLTP Data Store): <ul> <li>Row store, </li> <li>low latency, </li> <li>High Throughput, </li> <li>Concurrent, </li> <li>High Velocity, </li> <li>Caching</li> </ul> </li> <li>Analytical Data Store: Divided into OLAP and Decision Support System (DSS). <ul> <li>Columnar store</li> <li>Large data sets, Partioned</li> <li>Large Compute Size</li> <li>Regularly performs complex joins and aggregations</li> <li>Bulk Loading or Trickle Inserts</li> <li>Low Change Velocity</li> </ul> </li> <li>OLAP: OLAP systems provide a more responsive framework for real-time feedback and ad-hoc queries. </li> <li>DSS Systems: DSS systems, such as data lakes and data warehouses, are useful for long-running query aggregations and projections, where latency is less important.</li> </ul>"},{"location":"storage/storage.html#oltp-stores","title":"OLTP stores","text":"<ul> <li>RDS: Scales vertically. Reliable and durable: Multi-AZ and automated backups, snapshots, and failover.</li> <li>DynamoDB: Scales horizontally. Reliable and durable. Offers global tables for multi-region replication.</li> <li>ElastiCache: Extreme performance. ElastiCache for Redis offers multi-az storage with automatic failover.</li> <li>Neptune: Fast and scalable (billions of relationships &amp; queries with milliseconds latency). Six replicas of data across three AZ. </li> </ul>"},{"location":"storage/storage.html#analytic","title":"Analytic","text":"<ul> <li>S3: Object store, fast &amp; reliable platform to store and query structured and semi-structured data. Athena &amp; Redshift can read from S3. Reliable and Durable (data replicated across 3 AZ, except for single AZ storage class), cross-region replication.</li> <li>Redshift: Columnar storage to improve I/O efficiency and parallelize queries. Data loads linearly. Fastest query results with higher storage cost. Reliable and durable: Continuously backs up your data to S3 (11 9s of durability)   </li> </ul>"},{"location":"storage/storage.html#dynamodb","title":"DynamoDB","text":"<ul> <li>Use DAX (DynamoDB compatible) for micro-second responses.</li> <li>Partition Key (or) Primary Key: Choose a column with high cardinality. User ID is good. Error code is bad.</li> <li>Secondary indexes: access pattern. These indexes can use alternate sort and/or primary keys to match secondary access patterns. <ul> <li>The careful design of secondary indexes allows fast and efficient access to data.</li> <li>Can choose local or global secondary indexes.</li> </ul> </li> <li>LSI: Same partition key, but different sort key. Both eventual and strong. Cannot be deleted, choose when you create table.</li> <li>GSI: Both partition and sort keys can be different. Global since the data can span all partitions. Only eventual consistency.</li> </ul> <p>In a DynamoDB table, there is no upper limit on the number of distinct sort key values per partition key value. If you needed to store many billions of Dog items in the Pets table, DynamoDB would allocate enough storage to handle this requirement automatically.</p>"},{"location":"storage/storage.html#local-secondary-index-lsi","title":"Local Secondary Index (LSI)","text":"<ul> <li>A local secondary index maintains an alternate sort key for a given partition key value.</li> <li>A local secondary index also contains a copy of some or all of the attributes from its base table. </li> <li>You specify which attributes are projected into the local secondary index when you create the table.</li> <li>LSI occupies additional space (for both indexes and projections)</li> <li>Queries, Scans and Updates consume read and write capacity units of the base table</li> </ul> <p>LSI is considered \u201clocal\u201d because every partition of a local secondary index is bounded by the same partition key value of the base table. It enables querying with different sort order.   </p> <p>If access patterns are known clearly upfront, and if strong read consistency is a requirement, then go for LSI.</p>"},{"location":"storage/storage.html#global-secondary-index-gsi","title":"Global Secondary Index (GSI)","text":"<ul> <li>To speed up queries on non-key attributes, you can create a global secondary index. </li> <li>A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. </li> <li>The index key does not need to have any of the key attributes from the table. It doesn't even need to have the same key schema as a table.</li> <li>Each index has its own provisioned throughput independent of base table</li> </ul> <p>GSI is deemed global because queries on the index can access the data across different partitions of the base table.</p> <p>Every global secondary index must have a partition key, and can have an optional sort key. The index key schema can be different from the base table schema.</p> <p>GSI is like having another table altogether.</p>"},{"location":"storage/storage.html#provisioned-capacity-mode","title":"Provisioned Capacity Mode","text":"<ul> <li>With provisioned capability mode, you specify the number of reads (RCUs) and writes (WCUs) per second that you expect your application to require. </li> <li>1 RCU represents one strongly consistent read or 2 eventually consistent reads per second for an item up to 4 KB in size. </li> <li>1 WCU represents one write per second for an item up to 1 KB in size. </li> <li>You can provision any amount of throughput to a table and use auto scaling to automatically adjust your table\u2019s capacity based on the specified utilization rate to ensure application performance while reducing costs.</li> </ul> <p>Provisioned capacity is probably best for you if you have relatively predictable application traffic, run applications whose traffic is consistent, and ramps up or down gradually.</p> <p>On-demand capacity mode is probably best when you have new tables with unknown workloads, unpredictable application traffic and also if you only want to pay exactly for what you use. The on-demand pricing model is ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or minutes, and when under-provisioned capacity would impact the user experience.</p>"},{"location":"storage/storage.html#global-tables","title":"Global Tables","text":"<ul> <li>Uses DynamoDB streams</li> </ul> <p>To help ensure eventual consistency, DynamoDB global tables use a last-writer-wins reconciliation between concurrent updates, in which DynamoDB makes a best effort to determine the last writer.</p>"},{"location":"storage/storage.html#backup","title":"Backup","text":"<ul> <li>AWS Backup</li> <li>DynamoDB on-demand backup. You can use the DynamoDB on-demand backup capability to create full backups of your tables for long-term retention, and archiving for regulatory compliance needs.</li> </ul>"},{"location":"storage/storage.html#redis","title":"Redis","text":"<ul> <li>You can use Redis Hashes to maintain a list of likes &amp; dislikes for a product code (product recommendation use case)</li> </ul>"},{"location":"storage/storage.html#s3","title":"S3","text":""},{"location":"storage/storage.html#s3-object-prefixes","title":"S3 Object Prefixes","text":"<ul> <li>You can use prefixes to organize the data that you store in Amazon S3 buckets. </li> <li>A prefix is a string of characters at the beginning of the object key name. </li> <li>A prefix can be any length, subject to the maximum length of the object key name</li> <li>Searching by prefix limits the results to only those keys that begin with the specified prefix.</li> </ul> <p>If your application issues requests directly to Amazon S3 using the REST API, we recommend using a pool of HTTP connections and re-using each connection for a series of requests. Avoiding per-request connection setup removes the need to perform TCP slow-start and Secure Sockets Layer (SSL) handshakes on each request. </p>"},{"location":"storage/storage.html#s3-select","title":"S3 Select","text":"<ul> <li>S3 Select enables applications to retrieve only a subset of data from an object by using simple SQL expressions. </li> <li>By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases (sometimes upto 400% improvement)</li> <li>Amazon Athena, Amazon Redshift, and Amazon EMR as well as partners like Cloudera, DataBricks, and Hortonworks will all support S3 Select.</li> </ul>"},{"location":"storage/storage.html#glacier-select","title":"Glacier Select","text":"<ul> <li>Some companies in highly regulated industries like Financial Services, Healthcare, and others, write data directly to Amazon Glacier to satisfy compliance needs like SEC Rule 17a-4 or HIPAA. </li> <li>Many S3 users have lifecycle policies designed to save on storage costs by moving their data into Glacier when they no longer need to access it on a regular basis. </li> <li>Cold data stored in Glacier can now be easily queried within minutes (vis-a-vis weeks with Tape solutions).</li> <li>Glacier Select allows you to to perform filtering directly against a Glacier object using standard SQL statements.</li> </ul>"},{"location":"storage/storage.html#glacier","title":"Glacier","text":"<ul> <li>To prevent deletion in Glacier, you must use a Glacier Vault Lock Policy.</li> <li>Glacier Select allows you to perform filtering directly against Glacier objects using standard SQL statements. It is the simplest, quickest, and most cost-effective option for querying cold data stored in Glacier.</li> </ul>"},{"location":"storage/storage.html#encryption","title":"Encryption","text":""},{"location":"storage/storage.html#sse-kms","title":"SSE-KMS","text":"<ul> <li>When you create an object, you can specify the use of server-side encryption with AWS Key Management Service (AWS KMS) keys to encrypt your data. This encryption is known as SSE-KMS. </li> <li>You can apply encryption when you are either uploading a new object or copying an existing object.</li> <li>SSE-KMS will allow you to use different KMS keys to encrypt the objects, and then you can grant users access to specific sets of KMS keys to give them access to the objects in S3 they should be able to decrypt.</li> </ul> <p>If you want to allow multiple users to decrypt and use different sets of objects (per each user), go with SSE-KMS and specific SSE-KMS key per each of these sets of objects. A bucket policy becomes complex in this case.</p>"},{"location":"storage/storage.html#large-files","title":"Large Files","text":"<ul> <li>To validate checksum of upload of large files, use the S3 ETag (sent in the response) against the local MD5 hash.</li> </ul>"},{"location":"storage/storage.html#redshift","title":"Redshift","text":""},{"location":"storage/storage.html#storage-options","title":"Storage Options","text":"<ul> <li>While Amazon RDS, DynamoDB, and Neptune are all built in SSDs, Amazon Redshift offers you a choice of SSD or HDD storage. </li> <li>Amazon Redshift offers three different node types to best accommodate your workloads. You can select RA3, DC2, or DS2, depending on your required performance and data size.<ul> <li>DC2: Compute Intensive with local SSD</li> <li>DS2: Legacy, uses HDD for lower cost, not recommended</li> <li>RA3: with managed storage, allows you to optimize your datawarehouse by scaling and paying for compute and storage independently. </li> </ul> </li> </ul> <p>By having the data located in both Amazon S3 and Redshift you are increasing the costs without a good reason. You can consider Redshift Spectrum instead. However, if you have a need that data will be modified using SQL for custom processing, store the data in Amazon Redshift database (Data Warehouse), rather than S3 (Data Lake).  </p>"},{"location":"storage/storage.html#nodes-and-slices","title":"Nodes and Slices","text":"<ul> <li>An Amazon Redshift cluster is a set of nodes. </li> <li>Each node in the cluster has its own operating system, dedicated memory, and dedicated disk storage. </li> <li>One node is the leader node, which manages the distribution of data and query processing tasks to the compute nodes. </li> <li>The compute nodes provide resources to do those tasks.</li> <li>The disk storage for a compute node is divided into a number of slices. </li> <li>The number of slices per node depends on the node size of the cluster. </li> <li>For example, each DS2.XL compute node has two slices, and each DS2.8XL compute node has 16 slices. </li> <li>The nodes all participate in running parallel queries, working on data that is distributed as evenly as possible across the slices.</li> <li>When the table is loaded with data, the rows are distributed to the node slices according to the distribution key that is defined for a table.</li> </ul> <p>To the extent that you anticipate where best to locate data initially, you can minimize the impact of data redistribution. Distribution styles: EVEN, KEY, ALL, AUTO. ALL distribution is appropriate only for relatively slow moving tables, since data is copied across nodes. </p> <p>If you intend to load data from a large, compressed file, we recommend that you split your data into smaller files that are about equal size, from 1 MB to 1 GB after compression. For optimum parallelism, the ideal file size is 1\u2013125 MB after compression. Make the number of files a multiple of the number of slices in your cluster.</p> <p>While you are not charged for delete markers, removing expired markers can improve the performance of LIST requests.</p>"},{"location":"storage/storage.html#data-distribution-styles","title":"Data Distribution styles","text":"<ul> <li>KEY: The rows are distributed according to the values in one column.</li> <li>EVEN: Data is distributed across nodes in a round-robin fashion. If the table does not participate in a join, use this.</li> <li>ALL: Data is copied across all nodes</li> <li>AUTO: Redshift decides.</li> </ul> <p>Sort Key: Interleaved: Increased Load and Vaccum times. Don't use interleaved in monotonously increasing attributes.</p>"},{"location":"storage/storage.html#workload-management-wlm","title":"Workload Management (WLM)","text":"<ul> <li>You can use workload management (WLM) to define multiple query queues and to route queries to the appropriate queues at runtime.</li> <li>For example, suppose that one group of users submits occasional complex, long-running queries that select and sort rows from several large tables. Another group frequently submits short queries that select only a few rows from one or two tables and run in a few seconds. In this situation, the short-running queries might have to wait in a queue for a long-running query to complete. </li> <li>WLM helps manage this situation by pushing these queries into separate queues.</li> </ul>"},{"location":"storage/storage.html#short-query-acceleration-sqa","title":"Short Query Acceleration (SQA)","text":"<ul> <li>Short query acceleration (SQA) prioritizes selected short-running queries ahead of longer-running queries. </li> <li>SQA runs short-running queries in a dedicated space, so that SQA queries aren't forced to wait in queues behind longer queries.</li> <li>If you enable SQA, you can reduce or eliminate workload management (WLM) queues that are dedicated to running short queries.</li> <li>In addition, long-running queries don't need to contend with short queries for slots in a queue, so you can configure your WLM queues to use fewer query slots.</li> </ul>"},{"location":"storage/storage.html#resizing","title":"Resizing","text":"<ul> <li>AWS generally recommends using elastic resize, assuming you can tolerate 10 minutes or so of read-only access. Elastic Resize works for both node count and type changes.</li> <li>Classic resize takes more time to complete, but it can be useful in cases where the change in node count or the node type to migrate to doesn't fall within the bounds for elastic resize. This can apply, for instance, when the change in node count is really large.</li> </ul> <p>As a general rule of thumb, choose elastic resize instead of classic resize (unless the change node count is really large).</p> <p>You can also use classic resize to change the cluster encryption. For example, you can use it to modify your unencrypted cluster to use AWS KMS encryption.</p> <p>To be sure that the cluster is available during a classic resize operation, copy the existing cluster.  Then, resize the new cluster. If data is written to the source cluster after a snapshot is taken, the data must be manually copied over.</p>"},{"location":"storage/storage.html#elastic-resize","title":"Elastic resize:","text":"<ul> <li>If elastic resize is available as an option, use elastic resize to change the node type, number of nodes, or both. Note that when you only change the number of nodes, the queries are temporarily paused and connections are kept open. An elastic resize takes between 10-15 minutes. During a resize operation, the cluster is read-only.</li> </ul>"},{"location":"storage/storage.html#classic-resize","title":"Classic resize:","text":"<ul> <li>Use classic resize to change the node type, number of nodes, or both. Choose this option when you're resizing to a configuration that isn't available through elastic resize. A resize operation takes two hours or more, or can take up to several days depending on your data size. During the resize operation, the source cluster is read-only.</li> </ul>"},{"location":"storage/storage.html#snapshot-restore-and-resize","title":"Snapshot, restore, and resize:","text":"<ul> <li>To be sure that the cluster is available during a classic resize operation, copy the existing cluster. Then, resize the new cluster. If data is written to the source cluster after a snapshot is taken, the data must be manually copied over. The manual data copy to the newly created target cluster must take place after the migration completes.</li> </ul>"},{"location":"storage/storage.html#fast-classic-resize","title":"Fast Classic Resize:","text":"<ul> <li>Fast classic resize is as quick as elastic resize and functions similar to classic resize. </li> <li>In this resize operation there are two main stages. In stage 1 (critical path), data is migrated from the source to a target cluster and the cluster is in read only mode. </li> <li>In stage 2 (off critical path) redistributing of the data, done in the previous data distribution style, is completed in the background. The duration of this stage depends on the volume to distribute and cluster workload.</li> </ul> <p>Classic resize: Use classic resize to change the node type, number of nodes, or both.</p>"},{"location":"storage/storage.html#copy-of-data","title":"Copy of Data","text":"<p>Amazon Redshift can automatically load in parallel from multiple compressed data files. However, if you use multiple concurrent COPY commands to load one table from multiple files, Amazon Redshift is forced to perform a serialized load. This type of load is much slower and requires a VACUUM process at the end if the table has a sort column defined. </p> <p>Splitting the files into 1-GB files and compressing them will improve performance of the COPY command.</p>"},{"location":"storage/storage.html#compression-encodings","title":"Compression encodings","text":"<ul> <li>Raw Encoding</li> <li>AZ64 Encoding</li> <li>Byte-Dictionary Encoding</li> <li>Delta Encoding</li> <li>LZO Encoding</li> <li>Mostly Encoding</li> <li>Runlength Encoding</li> <li>Text255 and Text32k Encodings</li> <li>Zstandard Encoding</li> </ul>"},{"location":"storage/storage.html#use-a-staging-table-to-perform-a-merge-upsert","title":"Use a staging table to perform a merge (upsert)","text":"<p>You can efficiently update and insert new data by loading your data into a staging table first.</p> <p>Amazon Redshift doesn't support a single merge statement (update or insert, also known as an upsert) to insert and update data from a single data source.  However, you can effectively perform a merge operation. To do so, load your data into a staging table and then join the staging table with your target table for an UPDATE statement and an INSERT statement.</p>"},{"location":"storage/storage.html#redshift-spectrum-overview","title":"Redshift Spectrum Overview","text":"<ul> <li>Amazon Redshift Spectrum resides on dedicated Amazon Redshift servers that are independent of your cluster. </li> <li>Amazon Redshift pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. </li> <li>Thus, Redshift Spectrum queries use much less of your cluster's processing capacity than other queries. </li> <li>Redshift Spectrum also scales intelligently. </li> <li>Based on the demands of your queries, Redshift Spectrum can potentially use thousands of instances to take advantage of massively parallel processing.</li> </ul> <p>Redshift Spectrum doesn't support update operations on external tables. Redshift Spectrum doesn't support Amazon EMR with Kerberos.</p>"},{"location":"storage/storage.html#rdms","title":"RDMS","text":"<ul> <li>If you are working on an on-premise PostgreSQL database, you can use DMS (Database Management Service) to replicate data to RDS, and offload analytical queries on it.</li> </ul> <p>If you are working with Amazon Aurora, read replicas help to offload analytical queries execution.</p>"},{"location":"storage/storage.html#row-level-security","title":"Row-Level Security","text":"<ul> <li>Row-Level Security for IAM users is now supported in both RDS and DynamoDB. </li> <li>Web Identity Federation is required to allow end users to use their Federated accounts with IAM. </li> </ul>"},{"location":"storage/storage.html#dynamodb_1","title":"DynamoDB","text":"<ul> <li>DynamoDB Streams do not have direct integration with Firehose. If you need to move data from DynamoDB to ElasticSearch, you need to involve Lambda functions in the middle. </li> <li>DynamoDB has direct integration with Redshift. Use the COPY command to load data in parallel directly into an Amazon Redshift cluster. </li> </ul>"},{"location":"storage/storage.html#wcu-rcu","title":"WCU &amp; RCU","text":"<ul> <li>1 WCU = 1 KB/second. If you expect about 400 games to be written per second to your database, with each game emitting 80 KB of data, the WCU required is: 80 KB * 400 KB/second = 32000 WCU.</li> <li>1 RCU = 2 eventually consistent reads per second of 4 KB. 1800 eventually consistent reads per second will require 1800 * 80/8 = 18000 RCU.</li> </ul>"},{"location":"storage/storage.html#glue-data-catalog","title":"Glue Data Catalog","text":"<ul> <li>Unified metadata repository across RDBMS, RDS, Redshift and S3</li> <li>Track data evolution using schema versioning</li> <li>Query using Athena or Redshift Spectrum</li> <li>Apache Hive metastore compatible; can be used as an external metastore for applications running on Amazon EMR.</li> </ul>"},{"location":"storage/storage.html#catalog-steps","title":"Catalog steps","text":"<ol> <li>A crawler runs any custom classifiers that you choose to infer the format and schema of your data.</li> <li>The crawler connects to the data store.</li> <li>The inferred schema is created for your data.</li> <li>The crawler writes metadata to the AWS Glue Data Catalog.</li> </ol> <p>AWS Glue crawlers can automatically infer schema from source data in Amazon S3 and store the associated metadata in the Data Catalog. </p>"},{"location":"storage/storage.html#amazon-open-search","title":"Amazon Open Search","text":"<ul> <li>Each Elasticsearch index is split into some number of shards. </li> <li>You should decide the shard count before indexing your first document. </li> <li>The overarching goal of choosing a number of shards is to distribute an index evenly across all data nodes in the cluster</li> <li>However, these shards shouldn't be too large or too numerous.</li> </ul> <p>Improve the cluster performance by decreasing the number of shards of Amazon Elasticsearch index, if you are facing <code>JVMMemoryPressure</code> found in Amazon ES logs. </p>"},{"location":"storage/storage.html#dms","title":"DMS","text":"<ul> <li>Quickly and securely migrate databases to AWS, resilient, self healing</li> <li>You must create an EC2 instance to perform the replication tasks</li> </ul>"},{"location":"visualization/open_search.html","title":"Open Search","text":"<ul> <li>Amazon OpenSearch Service makes it easy for you to perform interactive log analytics, real-time application monitoring, a website search, and more. </li> <li>OpenSearch is an open-source, distributed search and analytics suite derived from Elasticsearch. </li> <li>Amazon OpenSearch Service is the successor to Amazon Elasticsearch Service and offers the latest versions of OpenSearch.</li> <li>Amazon Kinesis Firehose can ingest data into Open Search.</li> <li>Amazon OpenSearch Service provides an installation of OpenSearch Dashboards with every OpenSearch Service domain. </li> <li>OpenSearch Dashboard is a data aggregation and visualization tool that enables you to explore, visualize, analyze, and discover data in real-time with Amazon OpenSearch.</li> </ul>"},{"location":"visualization/open_search.html#open-search-dashboard","title":"Open Search Dashboard","text":"<ul> <li>The OpenSearch dashboard (an open-source visualization tool) is tightly integrated with Amazon OpenSearch. </li> <li>With the simple, browser-based interface OpenSearch Dashboards, you can create and share dynamic dashboards quickly. </li> <li>Using the OpenSearch dashboard is free, and you only have to pay for the infrastructure where you installed the software.</li> </ul>"},{"location":"visualization/open_search.html#performance","title":"Performance","text":""},{"location":"visualization/open_search.html#shards-vs-shard-size","title":"Shards vs. Shard Size","text":"<ul> <li>A good rule of thumb is to try to keep a shard size between 10\u201350 GiB.</li> <li>Large shards can make it difficult for ElasticSearch (ES) to recover from failure.</li> <li>Having too many small shards can cause performance issues and out of memory errors. </li> </ul> <p>Shards should be small enough that the underlying Amazon ES instance can handle them, but not so small that they place needless strain on the hardware.</p>"},{"location":"visualization/open_search.html#use-cases","title":"Use Cases","text":""},{"location":"visualization/open_search.html#document-search","title":"Document search","text":"<ul> <li>Extracting structured data from documents and creating a smart index using Amazon Elasticsearch Service (Amazon ES) allows you to search through millions of documents quickly. </li> <li>For example, a mortgage company could use Amazon Textract to process millions of scanned loan applications in a matter of hours and have the extracted data indexed in Amazon ES. </li> <li>This would allow them to create search experiences like searching for loan applications where the applicant's name is Jose Rizal, or searching for contracts where the interest rate is 2 percent.</li> </ul> <p>Redshift and Athena does not provide visual insights, unlike Kibana. Amazon Elasticsearch (Open Search) does not have a direct integration with Amazon QuickSight.</p>"},{"location":"visualization/open_search.html#real-time-transaction-data-analysis","title":"Real-Time Transaction Data Analysis","text":"<ul> <li>Kinesis Data Firehose --&gt; Redshift --&gt; Quicksight is a valid option for analytics. Although valid, this visualization solution is not capable of providing near-real-time data. <ul> <li>While data ingestion from Firehose to Redshift is supported, it will not be in near-real-time due to the way Firehose feeds data to Redshift. </li> <li>Behind the scenes, Firehose has to load the streaming data to Amazon S3 first and then issue a COPY command to move the data to Redshift. </li> <li>This method introduces latencies in the order of minutes.</li> </ul> </li> <li></li> </ul>"},{"location":"visualization/quick_sight.html","title":"Quicksight","text":""},{"location":"visualization/quick_sight.html#operational-characteristics","title":"Operational Characteristics","text":"<ul> <li>Cost<ul> <li>Amazon QuickSight has two different editions for pricing; standard edition and enterprise edition. </li> <li>Pricing is based on an annual subscription. </li> <li>Both standard and enterprise editions include SPICE (Super-fast, Parallel, and In-memory Calculation Engine) capacity, and you can get additional SPICE capacity for a monthly add on cost. </li> <li>Month to month billing options are available for both the editions.</li> </ul> </li> <li>Performance<ul> <li>Amazon QuickSight is built with SPICE. Built from the ground up for the cloud.</li> <li>SPICE uses a combination of columnar storage, in-memory technologies enabled through the latest hardware innovations, and machine code generation to run interactive queries on large datasets and get rapid responses.</li> </ul> </li> <li>Durability and availability<ul> <li>SPICE automatically replicates data for high availability and enables Amazon QuickSight to scale to hundreds of thousands of users who can all simultaneously perform fast interactive analysis across a wide variety of AWS data sources.</li> </ul> </li> <li>Scalability &amp; Elasticity<ul> <li>Fully managed service that auto-scales</li> </ul> </li> <li>Interfaces: Amazon QuickSight can connect to a wide variety of data sources including:<ul> <li>flat files (CSV, TSV, CLF, ELF)</li> <li>on-premises databases like SQL Server, MySQL, and PostgreSQL</li> <li>AWS data sources including Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon Athena and Amazon S3</li> <li>SaaS applications like Salesforce. </li> <li>You can also export analyzes from a visual to a file with CSV format.</li> </ul> </li> </ul> <p>Quicksight is integrated with CloudTrail, so all events including non-API events such as deleting dashboards can be stored through CloudTrail into S3.</p>"},{"location":"visualization/quick_sight.html#reader-sessions","title":"Reader Sessions","text":"<ul> <li>Amazon QuickSight Reader sessions are of 30-minute duration each. Each session is charged at $0.30 with maximum charges of $5 per Reader in a month.</li> <li>A Reader session starts with a user-initiated action (e.g., login, dashboard load, page refresh, drill-down or filtering) and runs for next 30-minutes. </li> <li>Keeping Amazon QuickSight open in a background browser window/tab does not result in active sessions until the Reader initiates action on page.</li> <li>Readers will only be logged out of QuickSight when their authentication expires, which is dependent on the authentication scheme in place (can be one of QuickSight-only users, SAML/Open ID Connect or Active Directory).</li> <li>A reader will be charged $0.30 a session up to a maximum of $5/month, after which the reader can access QuickSight at no charge for additional sessions.</li> </ul>"},{"location":"visualization/quick_sight.html#supported-visualizations","title":"Supported Visualizations","text":"<p>Amazon QuickSight supports assorted visualizations that facilitate different analytical approaches:</p> Category Charts Comparison and distribution Bar charts (several assorted variants) Changes over time Line graphs, Area line charts Correlation Scatter plots, Heat maps Aggregation Pie graphs, Tree maps Tabular Pivot tables show differences in data values across a geographical map Geo-spatial charts (maps) KPIs Comparison between a key-value and it's target value"},{"location":"visualization/quick_sight.html#kibana","title":"Kibana","text":"<p>Kibana is built into Amazon ES.</p>"},{"location":"visualization/quick_sight.html#active-directory-integration","title":"Active Directory Integration","text":"<p>Amazon QuickSight Enterprise edition supports both AWS Directory Service for Microsoft Active Directory and Active Directory Connector. Standard Edition does not support AD Connector.</p>"},{"location":"visualization/quick_sight.html#authentication","title":"Authentication","text":"<ul> <li>Amazon QuickSight Enterprise edition supports both AWS Directory Service for Microsoft Active Directory and Active Directory Connector.</li> </ul>"},{"location":"visualization/quick_sight.html#encryption","title":"Encryption","text":"<p>In Amazon QuickSight Enterprise edition, the data at rest in SPICE is encrypted using block-level encryption with AWS-managed keys. You cannot use customer-provided keys that are imported into AWS KMS.</p>"},{"location":"visualization/quick_sight.html#s3-integration","title":"S3 Integration","text":"<ul> <li>Datasets created using Amazon S3 as the data source are automatically imported into SPICE. </li> <li>The Amazon QuickSight console allows for the refresh of SPICE data on a schedule.</li> </ul> <p>You need to authorize Amazon QuickSight to access your S3 bucket from within the Amazon QuickSight Console.</p>"},{"location":"visualization/visualization.html","title":"Visualization","text":"<ul> <li>Kibana runs on port 5601 by default.</li> </ul>"},{"location":"visualization/visualization.html#types-of-charts","title":"Types of Charts","text":""},{"location":"visualization/visualization.html#heat-maps","title":"Heat Maps","text":"<ul> <li>Heat maps are pivot tables that highlight outliers and trends using color.</li> </ul>"},{"location":"visualization/visualization.html#types-of-ml-models","title":"Types of ML Models","text":"<ul> <li>If you are trying to predict categorical data, and if there are more than two categories, then it is a multi-class classification model.</li> </ul>"},{"location":"visualization/visualization.html#array-of-services","title":"Array of Services","text":"Service Description Athena Interactive query service for interactive analytics on data stored in Amazon S3 (uses Presto under the hood) OpenSearch Operational Analytics. Managed service that makes it easy to deploy &amp; operate OpenSearch clusters in the AWS Cloud EMR Managed cluster platform that simplifies running of Big Data Analytics workloads Kinesis Data Analytics Perform real-time time series analytics, feed real-time dashboards, and create real-time metrics. Delivery Firehose Fully managed service for delivering streaming data into Amazon S3, Redshift, OpenSearch or Splunk, without any need to write consumer code. Kinesis Video Streams Fully managed service to stream live video from devices to the AWS Cloud. Redshift Fully managed peta-byte size Data Warehouse service. SageMaker Fully managed Machine Learning Service Open Search Dashboard Open-source visualization tool designed to work with OpenSearch."},{"location":"visualization/visualization.html#amazon-quicksight","title":"Amazon Quicksight","text":""},{"location":"visualization/visualization.html#concepts","title":"Concepts","text":"Concept Description Sources AWS Services, Files, On-premises databases SPICE Superfast, Parrallel, In-Memory Calculation Engine Analysis Use Analysis to create and interact with visuals and stories. Visuals Graphical representation of a dataset using a type of graph ML Insights Propose potentially useful visuals based on evaluation of data Sheet A set of visuals that are viewed together Story A set of one or more scenes that you can play like a slideshow Dashboard A Read-Only snapshot of an analysis that you can share with other Quiksight users for reporting purposes."},{"location":"visualization/visualization.html#references","title":"References","text":"<ul> <li>Optimize Amazon EMR Clusters with EC2 Spot</li> </ul>"}]}